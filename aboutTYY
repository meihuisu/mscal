
https://www.alglib.net/interpolation/fastrbf.php

*** netcdf-4 is different from hdf5

netcdf-4 is simpler data model(dims,variables,attributes) with group and user-defined type
but it is built on top of HDF5

   pip install netCDF4
   read via,
     from netCDF4 import Dataset
     # Open the NetCDF file
     ncfile = Dataset('example.nc', 'r')  # 'r' means read-only
     # Print global attributes
     print("File description:", ncfile.description)
     # List all variables
     print("Variables:", ncfile.variables.keys())

hdf5 hierarchical model(dataset and group)

   pip install h5py
   read via,
     import h5.py
     # Open the HDF5 file in read-only mode
     with h5py.File('your_file.h5', 'r') as h5file:
     # List all top-level groups and datasets
     print("Keys in file:", list(h5file.keys())

##################
Asked copilot (which it is really good at this kind of tasks)

>>> why would people use netcdf-4 instead of hdf5 <<<

1. Standardization in Earth Sciences

NetCDF is a widely adopted standard in climate science, meteorology, oceanography, and remote sensing.
Many tools (e.g., NCO, CDO, Panoply, xarray, netCDF4-python) are built specifically to work with NetCDF data.

2. Simpler Data Model

NetCDF has a more constrained and predictable data model:

Dimensions, variables, attributes, and groups.


This makes it easier to interoperate across different tools and platforms.

3. Metadata Conventions

NetCDF supports CF (Climate and Forecast) conventions, which standardize metadata for geospatial data.
These conventions make NetCDF files self-describing, which is crucial for automated processing and sharing.

4. Backward Compatibility

NetCDF-4 can read older NetCDF formats (classic and 64-bit offset), while HDF5 cannot.
This is important for long-term data archives.

5. Interoperability

NetCDF files are often easier to share across institutions and software ecosystems.
Many GIS and scientific packages (e.g., ArcGIS, QGIS, MATLAB, R) have built-in support for NetCDF.

6. Simplified API

NetCDF APIs are generally simpler and more domain-specific than HDF5’s more general-purpose APIs.


When HDF5 Might Be Preferred

You need complex hierarchical structures, variable-length arrays, or advanced data types.
You’re working outside the Earth sciences domain.
You need fine-grained control over performance features like chunking and compression.
You’re building custom applications that benefit from HDF5’s flexibility.


##################


SFCVM datafile is in hdf5, with hierarchical structure with superblocks

CenCalTYY datafile is in netcdf-4 with very basic structure
     query,
       from scipy.interpolate import interpn
       ...
       varq = interpn(points, vari, pointq, method='linear', bounds_error=False, fill_value=np.nan)
     and varq is fished out out of vs/vp/rho block of data in .nc file

--

A C++ library that is similar to scipy.interpolate,
https://www.alglib.net/interpolation/fastrbf.php
using Radial Basis Function (RBF) model
with a C wrapper



#####################

Plan,

1) get non-parallel netcdf-4 and hdf5 stack working on UCVM
2) tyy code with embedded alglib and ucvm wrapper



